# Intro to PySpark

![PySpark Logo](./pysparklogo.png)

## What is Spark?

Apache Spark is a framework for large-scale data processing on 
single-node machines or clusters. Spark provides APIs in Scala, Java, Python, and R, as well as
tools like Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, 
GraphX for graph processing, and Structured Streaming for stream processing.
The Spark codebase is open-source and can be viewed [here](https://github.com/apache/spark).

## What is PySpark?

Python is the most popular language for data science, and for good reason.

It's easily readable, it has a rich ecosystem of packages and libraries. It's not just a powerful scripting language, 
but also supports high-level programming. Wes McKinney has a great summary of 
some of the [key benefits of Python for Big Data](https://wesmckinney.com/book/preliminaries#why_python).

This is where PySpark comes in: it allows you to leverage both the powers of both
Spark and Python!



